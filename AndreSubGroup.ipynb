{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreldeod/IDCE-302-Python-Programming/blob/main/AndreSubGroup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cluster steps to run Prithvi-100M for fairy-ring mapping of cranberry bogs."
      ],
      "metadata": {
        "id": "ycAq1zjCAlOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AndrÃ©"
      ],
      "metadata": {
        "id": "OUoNxkb1XNcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "\n",
        "My objective as this sub-group was to pre-process data and do an initial fine-tuning and prediction run of the Privthis-100M model on mapping fairy-ring pathogens in cranberry fields of New Jersey. My goal was to provide ready-to use input data and established workflows to facilitate further fine-tuning by the other subgroups. For the initial run I tried to stay as true to the parameters used in the fine-tuning configuration shared and explained by Sam Khallaghi. Sam, Professor Lyndon Estes and our TA, Rahebeh Abedi, provided an immense amount of help and guidance in this project. We also adapted code written by Hanxi Li for creating our image and label chips. Thank you!"
      ],
      "metadata": {
        "id": "a6Ct9MJMkZAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Methods\n",
        "###Data\n",
        "* We used an orthomosaic drone image of a cranberry farm in NJ from July 2015.\n",
        "* Labels were created by all members of the group.\n",
        "\n",
        "\n",
        "###Model\n",
        "* We used the [Prithvi-100M](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/README.md) foundation model.\n",
        "* The configuration file was an adaptation of Sam Khallaghi's fine-tuning configuration for mapping aquaculture.\n",
        "\n",
        "###Workflow\n",
        "1. [Set-up cluster environments.](https://github.com/kordi1372/cranberry/blob/main/notebooks/cluster_steps.ipynb) (Notebook contains steps for referencing coming from hls-foundation-os [repo](https://github.com/NASA-IMPACT/hls-foundation-os/tree/main))\n",
        "2. [Calculate class weights.](https://github.com/kordi1372/cranberry/blob/main/notebooks/class_weights.ipynb)\n",
        "3. [Chip input image and labels](https://github.com/kordi1372/cranberry/blob/main/notebooks/chipping.ipynb)\n",
        "    * See [ImageProcessing folder](https://github.com/kordi1372/cranberry/tree/main/cranberry/imageProcessing) for modules used in the chipping notebook.\n",
        "4. Use [configuration file](https://github.com/kordi1372/cranberry/blob/main/cranberry/cranberry_Andre_configuration.py) for fine-tuning the model.\n",
        "\n",
        "```\n",
        "mim train mmsegmentation /home/airg/adomingues/cranberry/models/cranberry_Andre.py\n",
        "```\n",
        "5. Use best saved epoch from fine-tuning to run model predictions on images to create a binary classification showing presence and absence of fairy rings.\n",
        "```\n",
        "python /home/airg/adomingues/cranberry/models/hls-foundation-os-main/model_inference.py -config /home/airg/adomingues/cranberry/models/cranberry_Andre_finetuned.py -ckpt /home/airg/adomingues/cranberry/outputs/run1_May4/best_mIoU_epoch_15.pth  -input /home/airg/adomingues/cranberry/data/chips_for_predictions/ -output /home/airg/adomingues/cranberry/outputs/run2_May6/ -input_type tif -bands 0 1 2 3 4 5\n",
        "```\n",
        "6. [Merge prediction chips](https://github.com/kordi1372/cranberry/blob/main/notebooks/merging_chips.ipynb) into a single binary classification raster.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xk_CDvmakd8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Working Directory:\n",
        "```\n",
        "cranberry -> data       -> prediction_chips\n",
        "                        -> labels\n",
        "                        -> images\n",
        "                        -> val_images.txt\n",
        "                        -> train_images.txt\n",
        "                                    \n",
        "          -> models     -> hls-foundation-os-main (from git clone)\n",
        "                        -> mmsegmentation (from git clone)\n",
        "                        -> cranberry_Andre.py\n",
        "                        -> Prithvi_100M.pt\n",
        "          \n",
        "          -> outputs    -> run_May7\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "98kt1JHDzAzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Results/analysis\n",
        "\n",
        "The first run of our fine-tune model was a bit unstable. Our best epoch was early on, at epoch 15 out of 80. The fairy ring class had an accuracy of 63.17, while non-fairy ring class had an accuracy of 82.72.\n",
        "\n",
        "The model was run with a fully convolutional network (FCN) encoder head. Cross entropy loss was used for the loss function. The class weights were calculated with the balance cross entropy loss function. We used small batches of 4 and a low running rate of 1.5e-05. We used AdamW for an adaptive optimizer.\n",
        "\n",
        "The prediction [image](https://github.com/kordi1372/cranberry/blob/main/cranberry/outputs/run_1_prediction_andre.png) from the first run, shows that the model did relatively well at identifying the fairy rings that were labeled. In some cases, like in the bottom left field, the model classified faint fairy rings that were missed by the labeler. There was more commission than commission. The model falsely classified forest canopy areas with lots of shadows as fairy rings in the north east corner of the image. The model did well at distinguishing the flooded cranberry bog and paved roads and fallow fields from fairy-rings. The model likely overclassified the fairy rings present in the bottom right. It is difficult to tell from visual inspection if the darker surfaces in this field are fairy ring related or not, but they lack the circular pattern.\n",
        "\n",
        "For future runs of Prithvi-100M on fairy ring cranberry mapping, it would be interesting to experiment with different decoder heads and loss functions to improve the models generalizability. The predictions should also be applied different ortho images.\n",
        "\n"
      ],
      "metadata": {
        "id": "mhiRXZl1kr2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Run 1 prediction](cranberry/outputs/run_1_prediction_andre.png \"Run 1 prediction\")\n",
        "\n",
        "Fairy ring classification. Presence in white, absence in black. Green outlines are the hand made labels. This prediction is from our first run of the fine-tuned model."
      ],
      "metadata": {
        "id": "98tKZuOPG1gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing metrics from fine-tunning the model\n",
        "---\n",
        "Epoch 5:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 64.41 | 88.71 |\n",
        "|   fairyring   | 46.88 | 53.42 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 72.92 | 55.65 | 71.06 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 15  (**BEST EPOCH**) :\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 63.72 | 82.72 |\n",
        "|   fairyring   | 52.06 | 63.17 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 73.97 | 57.89 | 72.95 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 30:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 61.89 | 84.55 |\n",
        "|   fairyring   | 46.01 | 54.79 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 71.23 | 53.95 | 69.67 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 45:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 60.81 | 84.58 |\n",
        "|   fairyring   | 43.44 | 51.71 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 69.88 | 52.13 | 68.15 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 60:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 60.37 | 82.56 |\n",
        "|   fairyring   | 44.93 |  54.6 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 70.05 | 52.65 | 68.58 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 75:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 59.78 | 80.98 |\n",
        "|   fairyring   | 45.51 |  56.2 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 69.89 | 52.65 | 68.59 |\n",
        "    +-------+-------+-------+\n",
        "```\n",
        "---\n",
        "Epoch 80:\n",
        "```\n",
        "+---------------+-------+-------+\n",
        "|     Class     |  IoU  |  Acc  |\n",
        "+---------------+-------+-------+\n",
        "| not_fairyring | 59.77 | 81.24 |\n",
        "|   fairyring   | 45.17 | 55.63 |\n",
        "+---------------+-------+-------+\n",
        "    +-------+-------+-------+\n",
        "    |  aAcc |  mIoU |  mAcc |\n",
        "    +-------+-------+-------+\n",
        "    | 69.78 | 52.47 | 68.44 |\n",
        "    +-------+-------+-------+\n",
        "```"
      ],
      "metadata": {
        "id": "6vKzyNVk99A-"
      }
    }
  ]
}